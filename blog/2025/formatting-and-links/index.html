<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Machine Learning Definitions | Umar M. Faruque </title> <meta name="author" content="Umar M. Faruque"> <meta name="description" content="Notes from various different sources/material related to ML"> <meta name="keywords" content="umar, umar faruque, umarhunter, umar faruque hunter, umar faruque hunter college,"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://umarhunter.github.io/blog/2025/formatting-and-links/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Umar</span> M. Faruque </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/academia/">academia </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Machine Learning Definitions</h1> <p class="post-meta"> Created on March 30, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ml</a>   ·   <a href="/blog/category/ml"> <i class="fa-solid fa-tag fa-sm"></i> ml</a> </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h3"><a href="#principal-component-analysis-pca">Principal Component Analysis (PCA)</a></li> <li class="toc-entry toc-h3"><a href="#dimension-reduction">Dimension Reduction</a></li> <li class="toc-entry toc-h3"><a href="#consolidation-and-embedding">Consolidation and Embedding</a></li> <li class="toc-entry toc-h3"><a href="#the-all-possible-subsets-method">The All Possible Subsets Method</a></li> <li class="toc-entry toc-h3"><a href="#grid-search">Grid Search</a></li> <li class="toc-entry toc-h3"><a href="#confidence-intervals">Confidence Intervals</a></li> <li class="toc-entry toc-h3"><a href="#regularization-shrinkage">Regularization (shrinkage)</a></li> </ul> </div> <hr> <div id="markdown-content"> <h3 id="principal-component-analysis-pca">Principal Component Analysis (PCA)</h3> <p>The Principal Component Analysis (PCA) is a tool that allows data scientists to reduce dimensionality of a large dataset using something called principal components (PC). PCA is not really a recent topic, it was created in 1901 by Karl Pearson. Modern machinery and algorithms enable us to use it today at scale.</p> <h3 id="dimension-reduction">Dimension Reduction</h3> <p>Some approaches to dimension reduction include consolidation and embedding, all-possible subsets, and the Principal Component Analysis (PCA).</p> <h3 id="consolidation-and-embedding">Consolidation and Embedding</h3> <p>While economists use proxy variables: say we want data on some variable U, but we don’t have data on it, we substitute another variable (let’s call that one V) which is very similar to U. V is a proxy of U.</p> <p>In ML, the related technique is called consolidation and embedding. For dimension reduction, proxies can be used to help reduce dimensionality of categorical variables.</p> <p>Encoding: when you take data from outside the data set to replace a feature</p> <h3 id="the-all-possible-subsets-method">The All Possible Subsets Method</h3> <p>When we want to choose a solid feature set, we can look at all posssible subsets of features. You’d find the MAPE or OME for each one and select the subset that minimizes that value.</p> <h3 id="grid-search">Grid Search</h3> <p>Many ML packages include the functionality to complete a <em>grid search</em>. Grid search means evaluating all possible hyperparameter combinations. Unfortunately, the number of combinations can be so large that a full grid search would take an extraordinarily long time to run.</p> <p>Some grid search packages attempt to rectify this by evaluating only promising combinations. To achieve this, they’d have to iteratively search through narrow parts of the grid. For each iteration, the algorithm updates its guess on what to try. This can save time but can, as a result, have the algorithm go the wrong direction.</p> <p>Another name for hyperparameters is <em>tuning parameters</em> – a pun referencing the old radio days when you’d literally tune the frequency to a precise station, known as fine tuning.</p> <h3 id="confidence-intervals">Confidence Intervals</h3> <p>Confidence intervals keep us honest, reminding us that values we see in certain ML functions are only approximate. When we construct a large # of CI’s, the overall validity declines. For example, CIs set individually at 95% level have a lower overall confidence level. The reason being is that each time you’re estimating something at 95% lowers the probability of it being 95% again, and so on.</p> <p>The <strong>Bonferroni-Dunn</strong> CI is often seen as a column alongside the normal CI. This gives us alternative CIs that take into consideration us having many random CIs. For example, the highest discrepancy the Bonferroni-Dunn CI has with the normal CI, the greater risk there is (that it’s not a valid number).</p> <h3 id="regularization-shrinkage">Regularization (shrinkage)</h3> <p>Without going into the math details (to spare me from typing LaTeX [no offense to any mathematicians]), I’ll type out most of the important bits.</p> <p>The <strong>James-Stein</strong> theory says the best estimate of mew (the estimator representing a population’s means) isn’t actually X-bar (the estimator representing a sample’s means), but a reduced version of X-bar. The reason is that samples can contain data that are outliers, so reducing it will actually take the outliers somewhat in account. <strong>The higher the dimensions the more shrinking needs to be done</strong>.</p> <p>In addition, different components of the vector can be shrunk by different amounts (including expansion), the reason being is that <em>shrinking</em> refers to the size of the entire vector, not a specific component. How much shrinking is done is usually decided by <strong>cross-validation</strong>.</p> <p>An additional reason why shrinkage/regularization has had a huge impact on statistics and machine learning is because its a tool that can be used to avoid overfitting. This can be a double edged sword, for example:</p> <ol> <li>We want to make the prediction sum of squares as small as possible (eliminating bias).</li> <li>The sum of squares can be optimistic (the values are usually very high). This can lead to the sum having a large variance, partly because of extreme data points. Shrinkage reduces variance because smaller quantities usually vary less than larger ones (balancing out the outliers).</li> </ol> <p>The hyperparameter lambda is used to control where we want to be between the aforementioned points. Overfitting may also occur if the hyperparameter is off.</p> <p><strong>To recap: shrinkage usually reduces variance, and we’ll accept the shrinkage if it can be done without increasing bias much. Regularization is used in not just linear models, but also in support vector machines, neural nets, PCA, etc.</strong></p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Umar M. Faruque. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>